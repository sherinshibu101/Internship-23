{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spektral\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "6JDYiBmiCzG0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IWdnZfS4atO",
        "outputId": "5abee685-6347-4bb5-bf46-659714b49d99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded the content file"
      ],
      "metadata": {
        "id": "MlorJtWs-lKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load .content file\n",
        "def load_content(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "    print(\"DataFrame shape:\", df.shape)\n",
        "    print(\"First few rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Check the first row to determine how many features are there\n",
        "    first_row = df.iloc[0].values\n",
        "    num_features = len(first_row) - 2  # Assuming last column is label and first is ID\n",
        "    column_names = ['node_id'] + [f'word_{i}' for i in range(num_features)] + ['label']\n",
        "    df.columns = column_names\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example file paths (adjust if necessary)\n",
        "content_file = '/content/drive/MyDrive/citeseer.content'\n",
        "\n",
        "# Load data\n",
        "content_df = load_content(content_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJfALctdEJWJ",
        "outputId": "8fa1ca3a-5d47-4a4e-8082-a954da8bdaca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame shape: (3312, 3705)\n",
            "First few rows of the DataFrame:\n",
            "     0     1     2     3     4     5     6     7     8     9     ...  3695  \\\n",
            "0  100157     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "1  100598     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2  105684     0     1     0     0     0     0     0     0     0  ...     0   \n",
            "3   11099     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "4  114091     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "\n",
            "   3696  3697  3698  3699  3700  3701  3702  3703    3704  \n",
            "0     0     0     0     0     0     0     0     0  Agents  \n",
            "1     0     0     0     0     0     0     0     0      IR  \n",
            "2     0     0     0     0     0     0     0     0  Agents  \n",
            "3     0     0     0     0     0     0     0     0      DB  \n",
            "4     0     0     0     0     0     0     0     0      AI  \n",
            "\n",
            "[5 rows x 3705 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4ad677c2c4c1>:5: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep='\\t', header=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processed the data and printed the output to verify"
      ],
      "metadata": {
        "id": "-mpNdUMr-54o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load content file into DataFrame\n",
        "def load_content(file_path):\n",
        "    # Specify dtype for the paper ID column to avoid mixed types warning\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, dtype={0: str})\n",
        "\n",
        "    # Extract paper IDs, features, and labels\n",
        "    paper_ids = df[0].values\n",
        "    features = df.iloc[:, 1:-1].values\n",
        "    labels = df.iloc[:, -1].values\n",
        "\n",
        "    return paper_ids, features, labels\n",
        "\n",
        "# Path to your content file\n",
        "file_path = '/content/drive/MyDrive/citeseer.content'\n",
        "paper_ids, features, labels = load_content(file_path)\n",
        "\n",
        "# Encode labels to integers\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "\n",
        "# Print the first few entries to verify\n",
        "print(\"Paper IDs:\", paper_ids[:5])\n",
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Labels:\", labels[:5])\n",
        "print(\"Encoded Labels:\", labels_encoded[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVxG17q99h3a",
        "outputId": "71ceef19-7eea-4ec6-c309-8a26be68ae9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper IDs: ['100157' '100598' '105684' '11099' '114091']\n",
            "Features shape: (3312, 3703)\n",
            "Labels: ['Agents' 'IR' 'Agents' 'DB' 'AI']\n",
            "Encoded Labels: [1 4 1 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded the cites dataset"
      ],
      "metadata": {
        "id": "dr1EbspA-_kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def load_cites(file_path):\n",
        "    G = nx.DiGraph()\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        cited, citing = line.strip().split()\n",
        "        G.add_edge(citing, cited)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Path to your cites file\n",
        "cites_file_path = '/content/drive/MyDrive/citeseer.cites'\n",
        "G = load_cites(cites_file_path)\n"
      ],
      "metadata": {
        "id": "JOf6lCdh9t5e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checked the no of papers in cites and contents file"
      ],
      "metadata": {
        "id": "3aOwBZmg_JMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "# Load content file into DataFrame\n",
        "def load_content(file_path):\n",
        "    # Specify dtype for the paper ID column to avoid mixed types warning\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, dtype={0: str})\n",
        "\n",
        "    # Extract paper IDs, features, and labels\n",
        "    paper_ids = df[0].values\n",
        "    features = df.iloc[:, 1:-1].values\n",
        "    labels = df.iloc[:, -1].values\n",
        "\n",
        "    return paper_ids, features, labels\n",
        "\n",
        "# Load cites file and create a graph\n",
        "def load_cites(file_path):\n",
        "    G = nx.DiGraph()\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        cited, citing = line.strip().split()\n",
        "        G.add_edge(citing, cited)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Path to your content file\n",
        "content_file_path = '/content/drive/MyDrive/citeseer.content'\n",
        "paper_ids, features, labels = load_content(content_file_path)\n",
        "\n",
        "# Path to your cites file\n",
        "cites_file_path = '/content/drive/MyDrive/citeseer.cites'\n",
        "G = load_cites(cites_file_path)\n",
        "\n",
        "# Get unique paper IDs from the content file\n",
        "paper_ids_from_content = set(paper_ids)\n",
        "\n",
        "# Get unique paper IDs from the cites file\n",
        "paper_ids_from_cites = set(G.nodes)\n",
        "\n",
        "# Find discrepancies\n",
        "missing_in_cites = paper_ids_from_content - paper_ids_from_cites\n",
        "missing_in_content = paper_ids_from_cites - paper_ids_from_content\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of papers in content file: {len(paper_ids_from_content)}\")\n",
        "print(f\"Number of papers in cites file: {len(paper_ids_from_cites)}\")\n",
        "print(f\"Missing in cites file: {missing_in_cites}\")\n",
        "print(f\"Missing in content file: {missing_in_content}\")\n",
        "\n",
        "# If needed, check individual counts\n",
        "print(f\"Number of missing IDs in cites file: {len(missing_in_cites)}\")\n",
        "print(f\"Number of missing IDs in content file: {len(missing_in_content)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OINnrahXwt-",
        "outputId": "ef550c19-96a5-4594-9204-8029676cdf17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers in content file: 3312\n",
            "Number of papers in cites file: 3327\n",
            "Missing in cites file: set()\n",
            "Missing in content file: {'kohrs99using', 'hahn98ontology', '293457', 'ghani01hypertext', 'wang01process', 'gabbard97taxonomy', 'raisamo99evaluating', '38137', 'nielsen00designing', 'khardon99relational', 'flach99database', 'weng95shoslifn', 'tobies99pspace', '95786', '197556'}\n",
            "Number of missing IDs in cites file: 0\n",
            "Number of missing IDs in content file: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removed the missing papers from the cites file"
      ],
      "metadata": {
        "id": "yj6BxXHW_QKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "# Load content file into DataFrame\n",
        "def load_content(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, dtype={0: str})\n",
        "    paper_ids = df[0].values\n",
        "    features = df.iloc[:, 1:-1].values\n",
        "    labels = df.iloc[:, -1].values\n",
        "    return paper_ids, features, labels\n",
        "\n",
        "# Load citation file into a directed graph\n",
        "def load_cites(file_path):\n",
        "    G = nx.DiGraph()\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    for line in lines:\n",
        "        cited, citing = line.strip().split()\n",
        "        G.add_edge(citing, cited)\n",
        "    return G\n",
        "\n",
        "# Identify missing IDs in content file\n",
        "def identify_missing_ids(content_ids, cites_graph):\n",
        "    cites_ids = set(cites_graph.nodes())\n",
        "    content_ids_set = set(content_ids)\n",
        "    missing_ids = cites_ids - content_ids_set\n",
        "    return missing_ids\n",
        "\n",
        "# Remove missing IDs from the citation graph\n",
        "def remove_missing_ids_from_graph(graph, missing_ids):\n",
        "    graph.remove_nodes_from(missing_ids)\n",
        "\n",
        "# Paths to your files\n",
        "content_file_path = '/content/drive/MyDrive/citeseer.content'\n",
        "cites_file_path = '/content/drive/MyDrive/citeseer.cites'\n",
        "\n",
        "# Load data\n",
        "paper_ids, features, labels = load_content(content_file_path)\n",
        "G = load_cites(cites_file_path)\n",
        "\n",
        "# Identify missing IDs\n",
        "missing_ids = identify_missing_ids(paper_ids, G)\n",
        "print(f\"Missing IDs: {missing_ids}\")\n",
        "\n",
        "# Remove missing IDs from the graph\n",
        "remove_missing_ids_from_graph(G, missing_ids)\n",
        "\n",
        "# Verify the changes\n",
        "print(f\"Updated number of nodes: {G.number_of_nodes()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yApYlTSc7vAW",
        "outputId": "35e8f0bf-4e75-411b-8e9a-84c30ae76761"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing IDs: {'kohrs99using', 'hahn98ontology', '293457', 'ghani01hypertext', 'wang01process', 'gabbard97taxonomy', 'raisamo99evaluating', '38137', 'nielsen00designing', 'khardon99relational', 'flach99database', 'weng95shoslifn', 'tobies99pspace', '95786', '197556'}\n",
            "Updated number of nodes: 3312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Now we have to prepare the data\n",
        "  Create adjacency matrix , feature matrix and labels for this purpose"
      ],
      "metadata": {
        "id": "FAkvSHupADsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the content (node features and labels)\n",
        "def load_content(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, dtype={0: str})\n",
        "    paper_ids = df[0].values\n",
        "    features = df.iloc[:, 1:-1].values\n",
        "    labels = df.iloc[:, -1].values\n",
        "    return paper_ids, features, labels\n",
        "\n",
        "# Load the citation graph\n",
        "def load_cites(file_path):\n",
        "    G = nx.DiGraph()\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    for line in lines:\n",
        "        cited, citing = line.strip().split()\n",
        "        G.add_edge(citing, cited)\n",
        "    return G\n",
        "\n",
        "# Encode labels\n",
        "def encode_labels(labels):\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "    return labels_encoded\n",
        "\n",
        "# Create adjacency matrix\n",
        "def create_adjacency_matrix(graph, node_map):\n",
        "    n = len(node_map)\n",
        "    adjacency_matrix = np.zeros((n, n))\n",
        "    for citing, cited in graph.edges():\n",
        "        citing_idx = node_map[citing]\n",
        "        cited_idx = node_map[cited]\n",
        "        adjacency_matrix[citing_idx, cited_idx] = 1\n",
        "    return adjacency_matrix\n",
        "\n",
        "# Paths to your files\n",
        "content_file_path = '/content/drive/MyDrive/citeseer.content'\n",
        "cites_file_path = '/content/drive/MyDrive/citeseer.cites'\n",
        "\n",
        "# Load data\n",
        "paper_ids, features, labels = load_content(content_file_path)\n",
        "G = load_cites(cites_file_path)\n",
        "\n",
        "# Map paper IDs to node indices\n",
        "node_map = {paper_id: idx for idx, paper_id in enumerate(paper_ids)}\n",
        "\n",
        "# Remove missing nodes\n",
        "missing_ids = identify_missing_ids(paper_ids, G)\n",
        "remove_missing_ids_from_graph(G, missing_ids)\n",
        "\n",
        "# Re-map node indices after removing missing nodes\n",
        "node_map = {paper_id: idx for idx, paper_id in enumerate(G.nodes())}\n",
        "\n",
        "# Create adjacency matrix\n",
        "adjacency_matrix = create_adjacency_matrix(G, node_map)\n",
        "\n",
        "# Encode labels\n",
        "labels_encoded = encode_labels(labels)\n",
        "\n",
        "# Ensure feature matrix, adjacency matrix, and labels are aligned\n",
        "features = features[list(node_map.values())]\n",
        "labels_encoded = labels_encoded[list(node_map.values())]\n",
        "\n",
        "print(\"Adjacency Matrix:\", adjacency_matrix.shape)\n",
        "print(\"Feature Matrix:\", features.shape)\n",
        "print(\"Labels:\", labels_encoded.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyjd6XPo6qGP",
        "outputId": "918d27a3-86fe-41e7-d354-212991a6d7c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency Matrix: (3312, 3312)\n",
            "Feature Matrix: (3312, 3703)\n",
            "Labels: (3312,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0HZr6EU9QU5",
        "outputId": "16ffe642-a504-4ac1-9c0a-7a5828035450"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m667.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Convert adjacency matrix to edge index format\n",
        "def adjacency_to_edge_index(adj_matrix):\n",
        "    adj_matrix = torch.tensor(adj_matrix, dtype=torch.float)\n",
        "    edge_index = torch.nonzero(adj_matrix, as_tuple=False).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "def create_data_object(features, adjacency_matrix, labels):\n",
        "    edge_index = adjacency_to_edge_index(adjacency_matrix)\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data\n",
        "\n",
        "# Prepare your data\n",
        "data = create_data_object(features, adjacency_matrix, labels_encoded)\n"
      ],
      "metadata": {
        "id": "SS9K7CxX91W4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GATModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_channels * 8, out_channels, heads=1, dropout=0.6)\n",
        "        self.fc = torch.nn.Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.nn.functional.elu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "69G3bZK693gd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried to use SGD with momentum but it resulted in an accuracy of 54.1%\n"
      ],
      "metadata": {
        "id": "O_Gh5MhOAp1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RMSProp and it resulted in an accuracy of 94.8%"
      ],
      "metadata": {
        "id": "11lPAsxBBbQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "data_loader = DataLoader([data], batch_size=1, shuffle=True)\n",
        "\n",
        "# Instantiate the model, optimizer, and loss function\n",
        "model = GATModel(in_channels=features.shape[1], hidden_channels=64, out_channels=len(np.unique(labels_encoded)))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(200):  # Number of epochs\n",
        "    loss = train()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqRJnuCN96Nc",
        "outputId": "ed1d3c6c-1e21-45e1-ee7d-b4e4d8d5c3b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7988417148590088\n",
            "Epoch 2, Loss: 1.788718581199646\n",
            "Epoch 3, Loss: 1.7438023090362549\n",
            "Epoch 4, Loss: 1.7235617637634277\n",
            "Epoch 5, Loss: 1.5660966634750366\n",
            "Epoch 6, Loss: 1.5311999320983887\n",
            "Epoch 7, Loss: 1.48184335231781\n",
            "Epoch 8, Loss: 1.485579490661621\n",
            "Epoch 9, Loss: 1.455890417098999\n",
            "Epoch 10, Loss: 1.4554641246795654\n",
            "Epoch 11, Loss: 1.4280046224594116\n",
            "Epoch 12, Loss: 1.4234076738357544\n",
            "Epoch 13, Loss: 1.3884739875793457\n",
            "Epoch 14, Loss: 1.361904263496399\n",
            "Epoch 15, Loss: 1.3281476497650146\n",
            "Epoch 16, Loss: 1.3286356925964355\n",
            "Epoch 17, Loss: 1.3335047960281372\n",
            "Epoch 18, Loss: 1.3508355617523193\n",
            "Epoch 19, Loss: 1.3054454326629639\n",
            "Epoch 20, Loss: 1.279623031616211\n",
            "Epoch 21, Loss: 1.2830241918563843\n",
            "Epoch 22, Loss: 1.2805991172790527\n",
            "Epoch 23, Loss: 1.2698551416397095\n",
            "Epoch 24, Loss: 1.2644299268722534\n",
            "Epoch 25, Loss: 1.2294776439666748\n",
            "Epoch 26, Loss: 1.2464245557785034\n",
            "Epoch 27, Loss: 1.216057300567627\n",
            "Epoch 28, Loss: 1.2118021249771118\n",
            "Epoch 29, Loss: 1.2135412693023682\n",
            "Epoch 30, Loss: 1.2144527435302734\n",
            "Epoch 31, Loss: 1.2158753871917725\n",
            "Epoch 32, Loss: 1.20296311378479\n",
            "Epoch 33, Loss: 1.197167158126831\n",
            "Epoch 34, Loss: 1.192149043083191\n",
            "Epoch 35, Loss: 1.1721786260604858\n",
            "Epoch 36, Loss: 1.1498721837997437\n",
            "Epoch 37, Loss: 1.177469253540039\n",
            "Epoch 38, Loss: 1.1770002841949463\n",
            "Epoch 39, Loss: 1.1388345956802368\n",
            "Epoch 40, Loss: 1.155672550201416\n",
            "Epoch 41, Loss: 1.1445881128311157\n",
            "Epoch 42, Loss: 1.1463309526443481\n",
            "Epoch 43, Loss: 1.1220204830169678\n",
            "Epoch 44, Loss: 1.142982006072998\n",
            "Epoch 45, Loss: 1.130678653717041\n",
            "Epoch 46, Loss: 1.1196401119232178\n",
            "Epoch 47, Loss: 1.127508521080017\n",
            "Epoch 48, Loss: 1.124842643737793\n",
            "Epoch 49, Loss: 1.1185067892074585\n",
            "Epoch 50, Loss: 1.1464101076126099\n",
            "Epoch 51, Loss: 1.1207720041275024\n",
            "Epoch 52, Loss: 1.1202853918075562\n",
            "Epoch 53, Loss: 1.1089669466018677\n",
            "Epoch 54, Loss: 1.1012508869171143\n",
            "Epoch 55, Loss: 1.121494174003601\n",
            "Epoch 56, Loss: 1.1230127811431885\n",
            "Epoch 57, Loss: 1.109494686126709\n",
            "Epoch 58, Loss: 1.083117961883545\n",
            "Epoch 59, Loss: 1.0954896211624146\n",
            "Epoch 60, Loss: 1.107802152633667\n",
            "Epoch 61, Loss: 1.101995587348938\n",
            "Epoch 62, Loss: 1.0973615646362305\n",
            "Epoch 63, Loss: 1.096641182899475\n",
            "Epoch 64, Loss: 1.0948172807693481\n",
            "Epoch 65, Loss: 1.1053584814071655\n",
            "Epoch 66, Loss: 1.041130542755127\n",
            "Epoch 67, Loss: 1.092023491859436\n",
            "Epoch 68, Loss: 1.0750277042388916\n",
            "Epoch 69, Loss: 1.0754426717758179\n",
            "Epoch 70, Loss: 1.0778090953826904\n",
            "Epoch 71, Loss: 1.0872364044189453\n",
            "Epoch 72, Loss: 1.0830448865890503\n",
            "Epoch 73, Loss: 1.0742812156677246\n",
            "Epoch 74, Loss: 1.0815337896347046\n",
            "Epoch 75, Loss: 1.0903009176254272\n",
            "Epoch 76, Loss: 1.098126769065857\n",
            "Epoch 77, Loss: 1.066491723060608\n",
            "Epoch 78, Loss: 1.0829838514328003\n",
            "Epoch 79, Loss: 1.0677070617675781\n",
            "Epoch 80, Loss: 1.0505799055099487\n",
            "Epoch 81, Loss: 1.0587769746780396\n",
            "Epoch 82, Loss: 1.0678075551986694\n",
            "Epoch 83, Loss: 1.0705713033676147\n",
            "Epoch 84, Loss: 1.054530382156372\n",
            "Epoch 85, Loss: 1.089743733406067\n",
            "Epoch 86, Loss: 1.0843206644058228\n",
            "Epoch 87, Loss: 1.079377293586731\n",
            "Epoch 88, Loss: 1.0700933933258057\n",
            "Epoch 89, Loss: 1.0800172090530396\n",
            "Epoch 90, Loss: 1.0764274597167969\n",
            "Epoch 91, Loss: 1.0476371049880981\n",
            "Epoch 92, Loss: 1.0508488416671753\n",
            "Epoch 93, Loss: 1.0542017221450806\n",
            "Epoch 94, Loss: 1.0516037940979004\n",
            "Epoch 95, Loss: 1.0622824430465698\n",
            "Epoch 96, Loss: 1.0530786514282227\n",
            "Epoch 97, Loss: 1.0652828216552734\n",
            "Epoch 98, Loss: 1.0546646118164062\n",
            "Epoch 99, Loss: 1.0428130626678467\n",
            "Epoch 100, Loss: 1.0263670682907104\n",
            "Epoch 101, Loss: 1.069817304611206\n",
            "Epoch 102, Loss: 1.045861840248108\n",
            "Epoch 103, Loss: 1.0409027338027954\n",
            "Epoch 104, Loss: 1.0836366415023804\n",
            "Epoch 105, Loss: 1.0227285623550415\n",
            "Epoch 106, Loss: 1.0572199821472168\n",
            "Epoch 107, Loss: 1.0357555150985718\n",
            "Epoch 108, Loss: 1.0656273365020752\n",
            "Epoch 109, Loss: 1.0108364820480347\n",
            "Epoch 110, Loss: 1.0161277055740356\n",
            "Epoch 111, Loss: 1.056192398071289\n",
            "Epoch 112, Loss: 1.0680116415023804\n",
            "Epoch 113, Loss: 1.0580466985702515\n",
            "Epoch 114, Loss: 1.0477516651153564\n",
            "Epoch 115, Loss: 1.0380200147628784\n",
            "Epoch 116, Loss: 1.0352332592010498\n",
            "Epoch 117, Loss: 1.0521832704544067\n",
            "Epoch 118, Loss: 1.0497238636016846\n",
            "Epoch 119, Loss: 1.018261432647705\n",
            "Epoch 120, Loss: 1.0273733139038086\n",
            "Epoch 121, Loss: 1.0332698822021484\n",
            "Epoch 122, Loss: 1.0389479398727417\n",
            "Epoch 123, Loss: 1.0641701221466064\n",
            "Epoch 124, Loss: 1.0433756113052368\n",
            "Epoch 125, Loss: 1.0241929292678833\n",
            "Epoch 126, Loss: 1.0083166360855103\n",
            "Epoch 127, Loss: 1.0109628438949585\n",
            "Epoch 128, Loss: 1.0269304513931274\n",
            "Epoch 129, Loss: 1.0191091299057007\n",
            "Epoch 130, Loss: 1.025554895401001\n",
            "Epoch 131, Loss: 1.0325850248336792\n",
            "Epoch 132, Loss: 1.0248538255691528\n",
            "Epoch 133, Loss: 1.0207270383834839\n",
            "Epoch 134, Loss: 1.0519379377365112\n",
            "Epoch 135, Loss: 1.04860258102417\n",
            "Epoch 136, Loss: 1.0309303998947144\n",
            "Epoch 137, Loss: 1.0203949213027954\n",
            "Epoch 138, Loss: 1.033717393875122\n",
            "Epoch 139, Loss: 1.0162062644958496\n",
            "Epoch 140, Loss: 1.0468302965164185\n",
            "Epoch 141, Loss: 1.0207608938217163\n",
            "Epoch 142, Loss: 1.0164532661437988\n",
            "Epoch 143, Loss: 1.0149778127670288\n",
            "Epoch 144, Loss: 1.011071801185608\n",
            "Epoch 145, Loss: 1.0315216779708862\n",
            "Epoch 146, Loss: 1.0160740613937378\n",
            "Epoch 147, Loss: 1.0131388902664185\n",
            "Epoch 148, Loss: 1.024778127670288\n",
            "Epoch 149, Loss: 1.0244266986846924\n",
            "Epoch 150, Loss: 1.0240821838378906\n",
            "Epoch 151, Loss: 1.0220143795013428\n",
            "Epoch 152, Loss: 1.01524817943573\n",
            "Epoch 153, Loss: 1.022225260734558\n",
            "Epoch 154, Loss: 1.0355913639068604\n",
            "Epoch 155, Loss: 1.0158424377441406\n",
            "Epoch 156, Loss: 1.020567536354065\n",
            "Epoch 157, Loss: 1.03022301197052\n",
            "Epoch 158, Loss: 1.03255033493042\n",
            "Epoch 159, Loss: 1.0033336877822876\n",
            "Epoch 160, Loss: 1.018668293952942\n",
            "Epoch 161, Loss: 1.0152275562286377\n",
            "Epoch 162, Loss: 1.0067156553268433\n",
            "Epoch 163, Loss: 1.0212042331695557\n",
            "Epoch 164, Loss: 1.0330709218978882\n",
            "Epoch 165, Loss: 0.9889822006225586\n",
            "Epoch 166, Loss: 1.0195770263671875\n",
            "Epoch 167, Loss: 0.9765146374702454\n",
            "Epoch 168, Loss: 1.0257540941238403\n",
            "Epoch 169, Loss: 1.0166524648666382\n",
            "Epoch 170, Loss: 1.0282453298568726\n",
            "Epoch 171, Loss: 1.0110820531845093\n",
            "Epoch 172, Loss: 1.0170187950134277\n",
            "Epoch 173, Loss: 1.025941252708435\n",
            "Epoch 174, Loss: 1.0111654996871948\n",
            "Epoch 175, Loss: 1.0075782537460327\n",
            "Epoch 176, Loss: 1.0103238821029663\n",
            "Epoch 177, Loss: 1.0228756666183472\n",
            "Epoch 178, Loss: 1.0003892183303833\n",
            "Epoch 179, Loss: 0.9942507743835449\n",
            "Epoch 180, Loss: 0.9897148609161377\n",
            "Epoch 181, Loss: 0.9936267733573914\n",
            "Epoch 182, Loss: 1.0026201009750366\n",
            "Epoch 183, Loss: 0.9933334589004517\n",
            "Epoch 184, Loss: 1.0075935125350952\n",
            "Epoch 185, Loss: 1.009131908416748\n",
            "Epoch 186, Loss: 0.9841095805168152\n",
            "Epoch 187, Loss: 0.9905111193656921\n",
            "Epoch 188, Loss: 0.9951040744781494\n",
            "Epoch 189, Loss: 1.024289608001709\n",
            "Epoch 190, Loss: 0.9901886582374573\n",
            "Epoch 191, Loss: 1.0157506465911865\n",
            "Epoch 192, Loss: 1.0212998390197754\n",
            "Epoch 193, Loss: 1.027785062789917\n",
            "Epoch 194, Loss: 0.9980183243751526\n",
            "Epoch 195, Loss: 1.0080416202545166\n",
            "Epoch 196, Loss: 1.0005943775177002\n",
            "Epoch 197, Loss: 1.0425033569335938\n",
            "Epoch 198, Loss: 1.0027495622634888\n",
            "Epoch 199, Loss: 1.0111522674560547\n",
            "Epoch 200, Loss: 1.0157157182693481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "    return pred\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = evaluate(data)\n",
        "print(f'Predictions: {predictions}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_q1RGUf9_Xi",
        "outputId": "6b35db6b-d7bb-472f-df21-263e6566e76f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: tensor([1, 4, 1,  ..., 4, 2, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a LabelEncoder instance used earlier\n",
        "predicted_labels = label_encoder.inverse_transform(predictions.numpy())\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D14HcrAT-LiQ",
        "outputId": "7718e2bf-9cee-408d-8e8f-d128a16d1fb3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Agents' 'IR' 'Agents' ... 'IR' 'DB' 'ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(labels_encoded, predictions.numpy())\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(labels_encoded, predictions.numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeaTF0Nd-Nan",
        "outputId": "460e34e4-8bbf-4539-daee-18d59a88eb1b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9746376811594203\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       249\n",
            "           1       0.96      0.98      0.97       596\n",
            "           2       0.99      0.98      0.98       701\n",
            "           3       0.98      0.97      0.98       508\n",
            "           4       0.97      0.97      0.97       668\n",
            "           5       0.98      0.97      0.98       590\n",
            "\n",
            "    accuracy                           0.97      3312\n",
            "   macro avg       0.97      0.98      0.97      3312\n",
            "weighted avg       0.97      0.97      0.97      3312\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got an accuracy of 97.4% using Adam optimizers and observed as increasing the no of epochs the accuracy increased rapidly from 54 to 94 when increased from 10 to 100"
      ],
      "metadata": {
        "id": "4kV06YxoDWDm"
      }
    }
  ]
}